#CUDA_VISIBLE_DEVICES=1 python main.py --use_esposalles --use_parzival --use_washington --use_iam --model_architecture vit_atienza --image_height 224 --tokenizer_name oda_giga_tokenizer --batch_size 128 --square_image_max_size 224 --loss_function ctc --num_workers_train 4 --num_workers_test 4 --epoches 75 --learning_rate 0.0001 --reduce_on_plateau 5 --decoder_architecture transformer --output_model_name handwritten_expert --load_checkpoint --checkpoint_name /data/users/amolina/oda_ocr_output/non_linear_hiertext_only_base/non_linear_hiertext_only_base.pt
CUDA_VISIBLE_DEVICES=2 python main.py --use_mlt --mlt19_langs Latin --use_cocotext --use_textocr --model_architecture vit_atienza --image_height 224 --tokenizer_name oda_giga_tokenizer --batch_size 128 --square_image_max_size 224 --loss_function ctc --num_workers_train 4 --num_workers_test 4 --epoches 75 --learning_rate 0.0001 --reduce_on_plateau 5 --decoder_architecture transformer --output_model_name scene_expert --load_checkpoint --checkpoint_name /data/users/amolina/oda_ocr_output/non_linear_hiertext_only_base/non_linear_hiertext_only_base.pt
CUDA_VISIBLE_DEVICES=2 python main.py --use_mlt --mlt19_langs Latin --use_cocotext --use_textocr --use_esposalles --use_parzival --use_washington --use_iam --model_architecture vit_atienza --image_height 224 --tokenizer_name oda_giga_tokenizer --batch_size 128 --square_image_max_size 224 --loss_function ctc --num_workers_train 4 --num_workers_test 4 --epoches 75 --learning_rate 0.0001 --reduce_on_plateau 5 --decoder_architecture transformer --output_model_name both_hw_scene_expert --load_checkpoint --checkpoint_name /data/users/amolina/oda_ocr_output/non_linear_hiertext_only_base/non_linear_hiertext_only_base.pt
